{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/lingchm/datascience/blob/master/exercises/socially_distanced_robots.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVWkxKm9TYyT"
   },
   "source": [
    "# Implementing Logistic Regression\n",
    "\n",
    "*convex optimization*, *logistic regression*, *gradient descent*, *heavy ball*, *nesterov*, *newton's method*\n",
    "\n",
    "**Problem**\n",
    "\n",
    "Logistic regression is a simple, but surprisingly powerful, tool for solving a fundamental problem in machine learning: learning a classiâ€€er that can distinguish between two classes from a set of training data. \n",
    "\n",
    "Suppose that we have a set of training data $(x_1, y_1), \\dots, (x_N, y_N)$, where each $x_n \\in \\mathbb{R}^D$ is a feature vector and $y_n \\in \\{0,1\\}$ is a label of two classes.\n",
    "\\feature vector\" and each yn 2 f0; 1g is a \\label\" that indicates which of two classes. The goal in learning a classifier is to find a function $h(x)$ that predicts the correct label for a feature vector that we have never seen before. We can do this by trying to learn a function $h$ that gives us $h(x_n) = y_n$ on (most of) the training set of $N$ samples. \n",
    "\n",
    "Consider the logistic function\n",
    "$$\n",
    "g(t) = \\frac{1}{1 + e^{-t}} \\in [0, 1]\n",
    "$$\n",
    "\n",
    "In logistic regression, we set $t = a^Tx + b$ where $a \\in \\mathbb{R}^D$ and $b \\in \\mathbb{R}$, so that for any new sample $x$ we can compute $g(a^T x+b)$, returning a number between 0 and 1, interpreted as the probability of belonging to one of the classes. To form a classifier, we then simply compare the predicted probability with a threshold. A simple threshold is 0.5. \n",
    "\n",
    "To learn $a$ and $b$, we can setup the following optimization problem\n",
    "$$\n",
    "\\max_{a,b} \\sum_{n: y_n=1} \\log( g(a^Tx_n +b)) +  \\sum_{n: y_n=0} \\log( 1-g(a^Tx_n +b)).\n",
    "$$\n",
    "This can be simplified as\n",
    "$$\n",
    "max_{a,b} \\sum^N_{n=1} y_n(a^Tx_n +b)) - \\log( 1 + e^{a^Tx_n +b}).\n",
    "$$\n",
    "\n",
    "\n",
    "**Method**\n",
    "\n",
    "We explore four alternative approaches for solving this problem:\n",
    "1. Gradient descent\n",
    "2. Heavy ball\n",
    "3. Nesterov\n",
    "4. Newton's method\n",
    "\n",
    "Results are compared with Scikitlearn's logistic regression function.\n",
    "\n",
    "We implement each method with three approaches\n",
    "1. with fixed step size \n",
    "2. using backtracking to find step size\n",
    "3. using bisection to find step size\n",
    "\n",
    "**References**\n",
    "\n",
    "Credits to Dr. Justin Romberg for designing this problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "naVMsakRO7je"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "from sklearn import datasets\n",
    "from numpy.linalg import inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "np.random.seed(2020) \n",
    "x, y = datasets.make_blobs(n_samples=100, n_features=2, centers=2, cluster_std=6.0)\n",
    "x_ = np.concatenate((x, np.ones((x.shape[0], 1))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "osBCIqX-Ztvh"
   },
   "outputs": [],
   "source": [
    "# function to compute objective function, its gradient, and hessian matrix\n",
    "def obj_function(x, x_, y):\n",
    "    t = np.dot(x_, x)\n",
    "    return np.dot(y, t) - np.sum(np.log(1 + np.exp(t)), axis=0)\n",
    "    \n",
    "def gradf(x, x_, y):\n",
    "    e = np.exp(np.dot(x_, x))\n",
    "    return np.sum(np.multiply(np.tile(y - e / ( 1 + e), \n",
    "                  (x_.shape[1], 1)).T, x_), axis=0)\n",
    "\n",
    "def hessianf(x, x_):\n",
    "    t = np.zeros((x_.shape[1], x_.shape[1]))\n",
    "    for i in range(x_.shape[0]):\n",
    "        e = np.exp(np.dot(x_[i, :], x))\n",
    "        t += e / (e + 1)**2 * (x_[i, :].reshape(-1, 1) @ x_[i, :].reshape(1, -1))\n",
    "    return t    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two ways to find step size\n",
    "def find_alpha_bisection(xk, dk, y, x_, epsilon=1e-3):\n",
    "    al, ah, alpha, alpha_old = epsilon, 0.1, 0.001, 1\n",
    "    k = 0\n",
    "    while np.absolute(alpha - alpha_old) > epsilon:\n",
    "        alpha_old = 1 * alpha\n",
    "        alpha = (al + ah) / 2\n",
    "        e = np.exp(np.dot(x_, theta + alpha * dk)) # 100x1 \n",
    "        a_gradient = np.sum(np.multiply(y, np.dot(dk.T, x_.T))) + \\\n",
    "            np.sum(np.multiply(np.dot(x_, dk), e / (1 + e)))\n",
    "        if a_gradient > 0:\n",
    "            ah = alpha * 1\n",
    "        elif a_gradient < 0:\n",
    "            al = alpha * 1\n",
    "        else:\n",
    "            return alpha\n",
    "        k += 1\n",
    "    #print(\"number of iterations of bisection:\", k, \"alpha: \", alpha)\n",
    "    return alpha, k\n",
    "\n",
    "def find_alpha_backtracking(xk, dk, a0, c1, p, x_, y, tol=1e-6):\n",
    "    alpha, k = a0, 1\n",
    "    while obj_function(xk + alpha * dk, x_, y) < (obj_function(xk, x_, y) + \\\n",
    "               c1 * alpha * np.dot(gradf(xk, x_, y), dk)):\n",
    "        alpha *= p\n",
    "        k += 1\n",
    "    return alpha, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "xBikm2tiZw_e"
   },
   "outputs": [],
   "source": [
    "# Solution 1: Gradient descent\n",
    "def gradient_descent(y, x_, alpha=0.001, c1=0.5, p=0.5, tol=1e-6):\n",
    "    \n",
    "    k, k_, max_iter, dk = 0, 0, 10000, 1\n",
    "    theta = np.zeros(x_.shape[1])\n",
    "    while np.linalg.norm(dk) > tol and k < max_iter:\n",
    "        dk = gradf(theta, x_, y) \n",
    "        if alpha == 'bisection':\n",
    "            ak, i = find_alpha_bisection(theta, dk, y, x_)\n",
    "            k_ += i\n",
    "        elif alpha == 'backtracking':\n",
    "            ak, i = find_alpha_backtracking(theta, dk, 0.1, c1, p, x_=x_, y=y)\n",
    "            k_ += i                \n",
    "        else:\n",
    "            ak = alpha\n",
    "            i = 0\n",
    "        theta += ak * dk\n",
    "        k += 1\n",
    "        \n",
    "    print(\"Number of iterations of gradient descent:\", k)\n",
    "    print(\"Number of iterations of each bisection:\", i)\n",
    "    print(\"Number of iterations of bisection:\", k_)\n",
    "    print(\"Number of iterations:\", k_ + k)\n",
    "    print(\"Estimated theta:\", theta)\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2: heavy ball\n",
    "def heavy_ball(y, x_, alpha=0.001, beta=0.95, c1=0.5, p=0.8, tol=1e-6):\n",
    "    \n",
    "    k, k_, max_iter, dk = 0, 0, 10000, 1\n",
    "    theta, theta_old = np.zeros(x_.shape[1]), np.ones(x_.shape[1])\n",
    "    while np.linalg.norm(dk) > tol and k < max_iter:\n",
    "        dk = gradf(theta, x_, y) \n",
    "        \n",
    "        if alpha == 'backtracking':\n",
    "            ak, i = find_alpha_backtracking(theta, dk, 0.1, c1, p, x_, y)\n",
    "            k_ += i                \n",
    "        else:\n",
    "            ak = alpha\n",
    "            i = 0\n",
    "        \n",
    "        dk += beta / ak * (theta - theta_old)\n",
    "            \n",
    "        theta_old = theta * 1\n",
    "        theta += ak * dk\n",
    "        k += 1\n",
    "        \n",
    "    print(\"Number of iterations of gradient descent:\", k)\n",
    "    print(\"Number of iterations of each bisection:\", i)\n",
    "    print(\"Number of iterations of bisection:\", k_)\n",
    "    print(\"Number of iterations:\", k_ + k)\n",
    "    print(\"Estimated theta:\", theta)\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "ioNaT09mZ2n-",
    "outputId": "03e8cc5d-484c-4a63-81c9-a407e3849eeb"
   },
   "outputs": [],
   "source": [
    "# Solution 3: Nestrov's method\n",
    "def nesterov(y, x_, alpha=0.001, beta=0.95, c1=0.5, p=0.5, tol=1e-6):\n",
    "    \n",
    "    k, k_, max_iter, dk = 0, 0, 10000, 1\n",
    "    theta, theta_old = np.zeros(x_.shape[1]), np.ones(x_.shape[1])\n",
    "    while np.linalg.norm(dk) > tol and k < max_iter:\n",
    "        dk = gradf(theta + beta * (theta - theta_old), x_, y) \n",
    "        beta = (k-1) / (k+2)\n",
    "        if alpha == 'backtracking':\n",
    "            ak, i = find_alpha_backtracking(theta, dk, 0.1, c1, p, x_=x_, y=y)\n",
    "            k_ += i                \n",
    "        else:\n",
    "            ak = alpha\n",
    "            i = 0\n",
    "        dk += beta / ak * (theta - theta_old)\n",
    "        theta_old = theta * 1\n",
    "        theta += ak * dk\n",
    "        k += 1\n",
    "        \n",
    "    print(\"Number of iterations of gradient descent:\", k)\n",
    "    print(\"Number of iterations of each bisection:\", i)\n",
    "    print(\"Number of iterations of bisection:\", k_)\n",
    "    print(\"Number of iterations:\", k_ + k)\n",
    "    print(\"Estimated theta:\", theta)\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 4: Newton's method\n",
    "def newton(y, x_, alpha=1, c1=0.5, p=0.5, tol=1e-6):\n",
    "    \n",
    "    k, k_, max_iter, dk = 0, 0, 10000, 1\n",
    "    theta = np.zeros(x_.shape[1])\n",
    "    while np.linalg.norm(dk) > tol and k < max_iter:\n",
    "        dk = inv(hessianf(theta, x_)) @ gradf(theta, x_, y) \n",
    "        if alpha == 'backtracking':\n",
    "            ak, i = find_alpha_backtracking(theta, dk, 1, c1, p, x_=x_, y=y)\n",
    "            k_ += i                \n",
    "        else:\n",
    "            ak = alpha\n",
    "            i = 0\n",
    "        theta += ak * dk\n",
    "        k += 1\n",
    "        \n",
    "    print(\"Number of iterations of gradient descent:\", k)\n",
    "    print(\"Number of iterations of each bisection:\", i)\n",
    "    print(\"Number of iterations of bisection:\", k_)\n",
    "    print(\"Number of iterations:\", k_ + k)\n",
    "    print(\"Estimated theta:\", theta)\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations of gradient descent: 3864\n",
      "Number of iterations of each bisection: 0\n",
      "Number of iterations of bisection: 0\n",
      "Number of iterations: 3864\n",
      "Estimated theta: [-0.2808693  -0.45756691  2.21342365]\n",
      "Classified 9 wrong out of 100\n",
      "Number of iterations of gradient descent: 2166\n",
      "Number of iterations of each bisection: 7\n",
      "Number of iterations of bisection: 15162\n",
      "Number of iterations: 17328\n",
      "Estimated theta: [-0.28086942 -0.45756708  2.21342507]\n",
      "Classified 9 wrong out of 100\n",
      "Number of iterations of gradient descent: 289\n",
      "Number of iterations of each bisection: 9\n",
      "Number of iterations of bisection: 3790\n",
      "Number of iterations: 4079\n",
      "Estimated theta: [-0.28087948 -0.45757267  2.21348641]\n",
      "Classified 9 wrong out of 100\n"
     ]
    }
   ],
   "source": [
    "theta = gradient_descent(y, x_, tol=1e-3)\n",
    "pred = np.where(np.dot(x_, theta) >= 0, 1, 0)\n",
    "print(\"Classified\", np.sum(np.absolute(pred - y)), \"wrong out of\", x.shape[0])\n",
    "\n",
    "theta = gradient_descent(y, x_, alpha='bisection', tol=1e-3)\n",
    "pred = np.where(np.dot(x_, theta) >= 0, 1, 0)\n",
    "print(\"Classified\", np.sum(np.absolute(pred - y)), \"wrong out of\", x.shape[0])\n",
    "\n",
    "# question 5 a\n",
    "theta = gradient_descent(y, x_, alpha='backtracking', c1=0.5, p=0.8, tol=1e-3)\n",
    "pred = np.where(np.dot(x_, theta) >= 0, 1, 0)\n",
    "print(\"Classified\", np.sum(np.absolute(pred - y)), \"wrong out of\", x.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations of gradient descent: 605\n",
      "Number of iterations of each bisection: 0\n",
      "Number of iterations of bisection: 0\n",
      "Number of iterations: 605\n",
      "Estimated theta: [-0.28091162 -0.45762542  2.21390607]\n",
      "Classified 9 wrong out of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lingchm/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations of gradient descent: 484\n",
      "Number of iterations of each bisection: 15\n",
      "Number of iterations of bisection: 8111\n",
      "Number of iterations: 8595\n",
      "Estimated theta: [-0.28092082 -0.45764652  2.21403439]\n",
      "Classified 9 wrong out of 100\n"
     ]
    }
   ],
   "source": [
    "theta = heavy_ball(y, x_, alpha=0.001, beta=0.95, tol=1e-3)\n",
    "pred = np.where(np.dot(x_, theta) >= 0, 1, 0)\n",
    "print(\"Classified\", np.sum(np.absolute(pred - y)), \"wrong out of\", x.shape[0])\n",
    "\n",
    "theta = heavy_ball(y, x_, alpha='backtracking', beta=0.95, c1=0.5, p=0.8, tol=1e-3)\n",
    "pred = np.where(np.dot(x_, theta) >= 0, 1, 0)\n",
    "print(\"Classified\", np.sum(np.absolute(pred - y)), \"wrong out of\", x.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations of gradient descent: 676\n",
      "Number of iterations of each bisection: 0\n",
      "Number of iterations of bisection: 0\n",
      "Number of iterations: 676\n",
      "Estimated theta: [-0.28140113 -0.45829609  2.21957098]\n",
      "Classified 9 wrong out of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lingchm/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations of gradient descent: 515\n",
      "Number of iterations of each bisection: 6\n",
      "Number of iterations of bisection: 22990\n",
      "Number of iterations: 23505\n",
      "Estimated theta: [-0.28077063 -0.45741223  2.21237208]\n",
      "Classified 9 wrong out of 100\n"
     ]
    }
   ],
   "source": [
    "theta = nesterov(y, x_, alpha=0.001, tol=1e-3)\n",
    "pred = np.where(np.dot(x_, theta) >= 0, 1, 0)\n",
    "print(\"Classified\", np.sum(np.absolute(pred - y)), \"wrong out of\", x.shape[0])\n",
    "\n",
    "theta = nesterov(y, x_, alpha='backtracking', beta=0.95, c1=0.8, p=0.9, tol=1e-3)\n",
    "pred = np.where(np.dot(x_, theta) >= 0, 1, 0)\n",
    "print(\"Classified\", np.sum(np.absolute(pred - y)), \"wrong out of\", x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations of gradient descent: 7\n",
      "Number of iterations of each bisection: 0\n",
      "Number of iterations of bisection: 0\n",
      "Number of iterations: 7\n",
      "Estimated theta: [-0.28091254 -0.45762617  2.21392367]\n",
      "Classified 9 wrong out of 100\n",
      "Number of iterations of gradient descent: 7\n",
      "Number of iterations of each bisection: 1\n",
      "Number of iterations of bisection: 7\n",
      "Number of iterations: 14\n",
      "Estimated theta: [-0.28091254 -0.45762617  2.21392367]\n",
      "Classified 9 wrong out of 100\n"
     ]
    }
   ],
   "source": [
    "theta = newton(y, x_, alpha=1, tol=1e-3)\n",
    "pred = np.where(np.dot(x_, theta) >= 0, 1, 0)\n",
    "print(\"Classified\", np.sum(np.absolute(pred - y)), \"wrong out of\", x.shape[0])\n",
    "\n",
    "theta = newton(y, x_, alpha=\"backtracking\", c1=0.5, p=0.8, tol=1e-3)\n",
    "pred = np.where(np.dot(x_, theta) >= 0, 1, 0)\n",
    "print(\"Classified\", np.sum(np.absolute(pred - y)), \"wrong out of\", x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified 9 wrong out of 100\n"
     ]
    }
   ],
   "source": [
    "### compare with scikit learn package\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(x, y)\n",
    "pred = model.predict(x)\n",
    "error = np.sum(np.absolute(pred - y))\n",
    "print(\"Classified\", error, \"wrong out of\", x.shape[0])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOTOc1U71AUuA5lNSIVuRuz",
   "include_colab_link": true,
   "name": "socially_distanced_robots.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
