{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P1_Bank Customer Churn Prediction.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lingchm/datascience-projects/blob/master/P1_Bank_Customer_Churn_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R88Ms0MTi0Ma"
      },
      "source": [
        "# Bank Customer Churn Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WA6lL1fni0Mb"
      },
      "source": [
        "In this project, we use supervised learning models to identify customers who are likely to churn in the future. Furthermore, we will analyze top factors that influence user retention. [Dataset information](https://www.kaggle.com/adammaus/predicting-churn-for-bank-customers)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bO94-bXZi0Md"
      },
      "source": [
        "## Contents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SIvRSRqAi0Md"
      },
      "source": [
        "\n",
        "* [Part 1: Data Exploration](#Part-1:-Data-Exploration)\n",
        "* [Part 2: Feature Preprocessing](#Part-2:-Feature-Preprocessing)\n",
        "* [Part 3: Model Training and Results Evaluation](#Part-3:-Model-Training-and-Result-Evaluation)\n",
        "* [Part 4: Feature Selection](#Part-4:-Feature-Selection)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TUoI2S7Bi6iR"
      },
      "source": [
        "# Part 0: Setup Google Drive Environment\n",
        "check this [link](https://colab.research.google.com/notebooks/io.ipynb) for more info"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "neechzbWi7rV",
        "colab": {}
      },
      "source": [
        "# install pydrive to load data\n",
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UScKyL2TjARW",
        "colab": {}
      },
      "source": [
        "link = 'https://drive.google.com/open?id=1eispPCMAQvWMcOF6M17LSoo06cMsnBUS'\n",
        "_, id = link.split('=')\n",
        "file = drive.CreateFile({'id':id}) \n",
        "file.GetContentFile('bank_churn.csv') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nK7A1qhYSDxM",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('bank_churn.csv')\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ikYHWOnYfKPM"
      },
      "source": [
        "**Homework 0: Use the recommended way to load data.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "1iLCfrlpe-pV",
        "colab": {}
      },
      "source": [
        "#@title Homework 0\n",
        "####\n",
        "## Your Code\n",
        "####\n",
        "import pandas as pd\n",
        "file_id='17nbs4OBtrj7kq40Lf9laopynyp4LJDGn'\n",
        "link='https://drive.google.com/uc?export=download&id={FILE_ID}'\n",
        "csv_url=link.format(FILE_ID=file_id)\n",
        "my_data = pd.read_csv(csv_url)\n",
        "my_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a6bG_gAPi0Me"
      },
      "source": [
        "# Part 1: Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bspx2K6fi0Me"
      },
      "source": [
        "### Part 1.1: Understand the Raw Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kuTHKjk-i0Mf",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "churn_df = pd.read_csv('bank_churn.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hHNZRs2Ti0Mi",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "churn_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C99Z9b7ai0Mm",
        "colab": {}
      },
      "source": [
        "print (\"Num of rows: \" + str(churn_df.shape[0])) # row count\n",
        "print (\"Num of columns: \" + str(churn_df.shape[1])) # col count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ht5YOBdx8NLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check data info\n",
        "churn_df.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZASeB8_089yA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check the unique values for each column\n",
        "churn_df.nunique()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4ec5r_Qdi0NL",
        "colab": {}
      },
      "source": [
        "# Get target variable\n",
        "y = churn_df['Exited']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rzCo_GC97rGd",
        "colab": {}
      },
      "source": [
        "# check the propotion of y = 1\n",
        "print(y.sum() / y.shape * 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SsAbAjhvi0Mx"
      },
      "source": [
        "### Part 1.2:  Understand the features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t1xsBp--_0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check missing values\n",
        "churn_df.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIqBIpOt_COM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# understand Numerical feature\n",
        "# discrete/continuous\n",
        "# 'CreditScore', 'Age', 'Tenure', 'NumberOfProducts'\n",
        "# 'Balance', 'EstimatedSalary'\n",
        "churn_df[['CreditScore', 'Age', 'Tenure', 'NumOfProducts','Balance', 'EstimatedSalary']].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSWC_9arxlfk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check the feature distribution\n",
        "# pandas.DataFrame.describe()\n",
        "# boxplot, distplot, countplot\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6o4PlZbuSYy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# boxplot for numerical feature\n",
        "_,axss = plt.subplots(2,3, figsize=[20,10])\n",
        "sns.boxplot(x='Exited', y ='CreditScore', data=churn_df, ax=axss[0][0])\n",
        "sns.boxplot(x='Exited', y ='Age', data=churn_df, ax=axss[0][1])\n",
        "sns.boxplot(x='Exited', y ='Tenure', data=churn_df, ax=axss[0][2])\n",
        "sns.boxplot(x='Exited', y ='NumOfProducts', data=churn_df, ax=axss[1][0])\n",
        "sns.boxplot(x='Exited', y ='Balance', data=churn_df, ax=axss[1][1])\n",
        "sns.boxplot(x='Exited', y ='EstimatedSalary', data=churn_df, ax=axss[1][2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZX_0E6R_E7e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# understand categorical feature\n",
        "# 'Geography', 'Gender'\n",
        "# 'HasCrCard', 'IsActiveMember'\n",
        "_,axss = plt.subplots(2,2, figsize=[20,10])\n",
        "sns.countplot(x='Exited', hue='Geography', data=churn_df, ax=axss[0][0])\n",
        "sns.countplot(x='Exited', hue='Gender', data=churn_df, ax=axss[0][1])\n",
        "sns.countplot(x='Exited', hue='HasCrCard', data=churn_df, ax=axss[1][0])\n",
        "sns.countplot(x='Exited', hue='IsActiveMember', data=churn_df, ax=axss[1][1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4DKTTdB6i0M2",
        "colab": {}
      },
      "source": [
        "# correlations between features\n",
        "corr_score = churn_df[['CreditScore', 'Age', 'Tenure', 'NumOfProducts','Balance', 'EstimatedSalary']].corr()\n",
        "\n",
        "# show heapmap of correlations\n",
        "sns.heatmap(corr_score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1qfEnNW_i0M5",
        "colab": {}
      },
      "source": [
        "# check the actual values of correlations\n",
        "corr_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aFa4d6t3i0NH"
      },
      "source": [
        "# Part 2: Feature Preprocessing\n",
        "\n",
        "feature encoding, feature scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JMTIEpY7IfPp"
      },
      "source": [
        "Read more for handling [categorical feature](https://github.com/scikit-learn-contrib/categorical-encoding), and there is an awesome package for [encoding](https://contrib.scikit-learn.org/categorical-encoding/#category-encoders)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "MrsjiqTwi0NQ",
        "colab": {}
      },
      "source": [
        "# ordinal encoding\n",
        "churn_df['Gender'] = churn_df['Gender'] == 'Female'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dx072uC1OJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# one hot encoding\n",
        "churn_df = pd.get_dummies(churn_df, columns=['Geography'], drop_first=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qMpX0j91IAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "churn_df.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sfa2fQx2xXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get feature space by dropping useless feature\n",
        "to_drop = ['RowNumber','CustomerId','Surname','Exited']\n",
        "X = churn_df.drop(to_drop, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wX0AEIgC3VCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q3x9ySX_i0Nd"
      },
      "source": [
        "# Part 3: Model Training and Result Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "77OjmSl9i0Nf"
      },
      "source": [
        "### Part 3.1: Split dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Uay8Md5li0Nh",
        "colab": {}
      },
      "source": [
        "# Splite data into training and testing\n",
        "from sklearn import model_selection\n",
        "\n",
        "# Reserve 20% for testing\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25, stratify = y)\n",
        "\n",
        "print('training data has ' + str(X_train.shape[0]) + ' observation with ' + str(X_train.shape[1]) + ' features')\n",
        "print('test data has ' + str(X_test.shape[0]) + ' observation with ' + str(X_test.shape[1]) + ' features')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JuPhtUkJi0NW",
        "colab": {}
      },
      "source": [
        "# Scale the data, using standardization\n",
        "# standardization (x-mean)/std\n",
        "# normalization (x-x_min)/(x_max-x_min) ->[0,1]\n",
        "\n",
        "# 1. speed up gradient descent\n",
        "# 2. same scale\n",
        "# 3. algorithm requirments\n",
        "\n",
        "# for example, use training data to train the standardscaler to get mean and std \n",
        "# apply mean and std to both training and testing data.\n",
        "# fit_transform does the training and applying, transform only does applying.\n",
        "# Because we can't use any info from test, and we need to do the same modification\n",
        "# to testing data as well as training data\n",
        "\n",
        "# https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py\n",
        "# https://scikit-learn.org/stable/modules/preprocessing.html\n",
        "\n",
        "\n",
        "# min-max example: (x-x_min)/(x_max-x_min)\n",
        "# [1,2,3,4,5,6] -> fit(min:1, max:6) (scalar.min = 1, scalar.max = 6) -> transform [(1-1)/(6-1),(2-1)/(6-1)..]\n",
        "# scalar.fit(train) -> min:1, max:100\n",
        "# scalar.transform(apply to x) -> apply min:1, max:100 to X_train\n",
        "# scalar.transform -> apply min:1, max:100 to X_test\n",
        "\n",
        "# scalar.fit -> mean:1, std:100\n",
        "# scalar.transform -> apply mean:1, std:100 to X_train\n",
        "# scalar.transform -> apply mean:1, std:100 to X_test\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c4UTtCQTi0Nl"
      },
      "source": [
        "### Part 3.2: Model Training and Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "EAhSxINLi0Nl",
        "colab": {}
      },
      "source": [
        "#@title build models\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Logistic Regression\n",
        "classifier_logistic = LogisticRegression()\n",
        "\n",
        "# K Nearest Neighbors\n",
        "classifier_KNN = KNeighborsClassifier()\n",
        "\n",
        "# Random Forest\n",
        "classifier_RF = RandomForestClassifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Av0IRSoBQ3pe",
        "colab": {}
      },
      "source": [
        "# Train the model\n",
        "classifier_logistic.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EiLuzUDJRBNi",
        "colab": {}
      },
      "source": [
        "# Prediction of test data\n",
        "classifier_logistic.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XjMV04mKRJ30",
        "colab": {}
      },
      "source": [
        "# Accuracy of test data\n",
        "classifier_logistic.score(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1OCgNSNri0Nn",
        "colab": {}
      },
      "source": [
        "# Use 5-fold Cross Validation to get the accuracy for different models\n",
        "model_names = ['Logistic Regression','KNN','Random Forest']\n",
        "model_list = [classifier_logistic, classifier_KNN, classifier_RF]\n",
        "count = 0\n",
        "\n",
        "for classifier in model_list:\n",
        "    cv_score = model_selection.cross_val_score(classifier, X_train, y_train, cv=5)\n",
        "    print(cv_score)\n",
        "    print('Model accuracy of ' + model_names[count] + ' is ' + str(cv_score.mean()))\n",
        "    count += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G1GCzaPxi0Np"
      },
      "source": [
        "#### Homework 1: Can you do prediction with SVM model?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "yxmKTdahi0Nq",
        "colab": {}
      },
      "source": [
        "#@title Homework 1\n",
        "####\n",
        "## Your Code\n",
        "####\n",
        "# SVC\n",
        "from sklearn.svm import SVC \n",
        "\n",
        "classifier_SVC = SVC()\n",
        "\n",
        "cv_score = model_selection.cross_val_score(classifier_SVC, X_train, y_train, cv=5)\n",
        "print('Model accuracy of SVM is: ' + str(cv_score.mean()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7J-23z78i0Ns"
      },
      "source": [
        "### (Optional) Part 3.3: Use Grid Search to Find Optimal Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Hpe9PEAAi0Nt",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# helper function for printing out grid search results \n",
        "def print_grid_search_metrics(gs):\n",
        "    print (\"Best score: \" + str(gs.best_score_))\n",
        "    print (\"Best parameters set:\")\n",
        "    best_parameters = gs.best_params_\n",
        "    for param_name in sorted(parameters.keys()):\n",
        "        print(param_name + ':' + str(best_parameters[param_name]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qvYo9I5Ti0Nv"
      },
      "source": [
        "#### Part 3.3.1: Find Optimal Hyperparameters - LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wOc48syxi0Nx",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# Possible hyperparamter options for Logistic Regression Regularization\n",
        "# Penalty is choosed from L1 or L2\n",
        "# C is the lambda value(weight) for L1 and L2\n",
        "\n",
        "# ('l1', 1) ('l1', 5) ('l1', 10) ('l2', 1) ('l2', 5) ('l2', 10)\n",
        "parameters = {\n",
        "    'penalty':('l1', 'l2'), \n",
        "    'C':(0.01, 0.1, 1, 5, 10)\n",
        "}\n",
        "Grid_LR = GridSearchCV(LogisticRegression(solver='liblinear'),parameters, cv=5)\n",
        "Grid_LR.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nN5rU0e-i0N1",
        "colab": {}
      },
      "source": [
        "# the best hyperparameter combination\n",
        "print_grid_search_metrics(Grid_LR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TtkDsXgui0N3",
        "colab": {}
      },
      "source": [
        "# best model\n",
        "best_LR_model = Grid_LR.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9u9YFedOi0N6"
      },
      "source": [
        "#### Part 3.3.2: Find Optimal Hyperparameters: KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o78422XVi0N6",
        "colab": {}
      },
      "source": [
        "# Possible hyperparamter options for KNN\n",
        "# Choose k\n",
        "parameters = {\n",
        "    'n_neighbors':[1,3,5,7,9] \n",
        "}\n",
        "Grid_KNN = GridSearchCV(KNeighborsClassifier(),parameters, cv=5)\n",
        "Grid_KNN.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ydaRZVAIi0N_",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# best k\n",
        "print_grid_search_metrics(Grid_KNN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq_qfVpXUJcx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_KNN_model = Grid_KNN.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nKn_oKLSi0OB"
      },
      "source": [
        "#### Part 3.3.3: Find Optimal Hyperparameters: Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NniAZIPfi0OC",
        "colab": {}
      },
      "source": [
        "# Possible hyperparamter options for Random Forest\n",
        "# Choose the number of trees\n",
        "parameters = {\n",
        "    'n_estimators' : [40,60,80]\n",
        "}\n",
        "Grid_RF = GridSearchCV(RandomForestClassifier(),parameters, cv=5)\n",
        "Grid_RF.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ScPiI-Bfi0OE",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# best number of tress\n",
        "print_grid_search_metrics(Grid_RF)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xJgfri_Mi0OG",
        "colab": {}
      },
      "source": [
        "# best random forest\n",
        "best_RF_model = Grid_RF.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xxDAOrGIi0OI"
      },
      "source": [
        "####Part 3.4: Model Evaluation - Confusion Matrix (Precision, Recall, Accuracy)\n",
        "\n",
        "class of interest as positive\n",
        "\n",
        "TP: correctly labeled real churn\n",
        "\n",
        "Precision(PPV, positive predictive value): tp / (tp + fp);\n",
        "Total number of true predictive churn divided by the total number of predictive churn;\n",
        "High Precision means low fp, not many return users were predicted as churn users. \n",
        "\n",
        "\n",
        "Recall(sensitivity, hit rate, true positive rate): tp / (tp + fn)\n",
        "Predict most postive or churn user correctly. High recall means low fn, not many churn users were predicted as return users."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o-tP94iFi0OI",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "# calculate accuracy, precision and recall, [[tn, fp],[]]\n",
        "def cal_evaluation(classifier, cm):\n",
        "    tn = cm[0][0]\n",
        "    fp = cm[0][1]\n",
        "    fn = cm[1][0]\n",
        "    tp = cm[1][1]\n",
        "    accuracy  = (tp + tn) / (tp + fp + fn + tn + 0.0)\n",
        "    precision = tp / (tp + fp + 0.0)\n",
        "    recall = tp / (tp + fn + 0.0)\n",
        "    print (classifier)\n",
        "    print (\"Accuracy is: \" + str(accuracy))\n",
        "    print (\"precision is: \" + str(precision))\n",
        "    print (\"recall is: \" + str(recall))\n",
        "\n",
        "# print out confusion matrices\n",
        "def draw_confusion_matrices(confusion_matricies):\n",
        "    class_names = ['Not','Churn']\n",
        "    for cm in confusion_matrices:\n",
        "        classifier, cm = cm[0], cm[1]\n",
        "        cal_evaluation(classifier, cm)\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(111)\n",
        "        cax = ax.matshow(cm, interpolation='nearest',cmap=plt.get_cmap('Reds'))\n",
        "        plt.title('Confusion matrix for ' + classifier)\n",
        "        fig.colorbar(cax)\n",
        "        ax.set_xticklabels([''] + class_names)\n",
        "        ax.set_yticklabels([''] + class_names)\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('True')\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OpSGaN49i0OL",
        "colab": {}
      },
      "source": [
        "# Confusion matrix, accuracy, precison and recall for random forest and logistic regression\n",
        "confusion_matrices = [\n",
        "    (\"Random Forest\", confusion_matrix(y_test,best_RF_model.predict(X_test))),\n",
        "    (\"Logistic Regression\", confusion_matrix(y_test,best_LR_model.predict(X_test))),\n",
        "    (\"K nearest neighbor\", confusion_matrix(y_test, best_KNN_model.predict(X_test)))\n",
        "]\n",
        "\n",
        "draw_confusion_matrices(confusion_matrices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyrlbl05xAO6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "confusion_matrix(y_test,best_RF_model.predict(X_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OvHlyhPBi0OT"
      },
      "source": [
        "### Part 3.4: Model Evaluation - ROC & AUC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jx_3XkgKi0OW"
      },
      "source": [
        "RandomForestClassifier, KNeighborsClassifier and LogisticRegression have predict_prob() function "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-Os_ZLTvi0OX"
      },
      "source": [
        "#### Part 3.4.1: ROC of RF Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UypvQMVBi0OY",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "from sklearn import metrics\n",
        "\n",
        "# Use predict_proba to get the probability results of Random Forest\n",
        "y_pred_rf = best_RF_model.predict_proba(X_test)[:, 1]\n",
        "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_rf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyADmHiPTtln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_RF_model.predict_proba(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s3PR-PdPi0Ob",
        "colab": {}
      },
      "source": [
        "# ROC curve of Random Forest result\n",
        "plt.figure(1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr_rf, tpr_rf, label='RF')\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve - RF model')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R89IUMYDi0Oe",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "# AUC score\n",
        "metrics.auc(fpr_rf,tpr_rf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-1DVqnJVi0Oh"
      },
      "source": [
        "#### Part 3.4.1: ROC of LR Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t-q5XJPoi0Oi",
        "colab": {}
      },
      "source": [
        "# Use predict_proba to get the probability results of Logistic Regression\n",
        "y_pred_lr = best_LR_model.predict_proba(X_test)[:, 1]\n",
        "fpr_lr, tpr_lr, thres = roc_curve(y_test, y_pred_lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zc4k8gUYcpNE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_LR_model.predict_proba(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KZSrN-1Mi0Ok",
        "colab": {}
      },
      "source": [
        "# ROC Curve\n",
        "plt.figure(1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr_lr, tpr_lr, label='LR')\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve - LR Model')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LHAyxishi0On",
        "colab": {}
      },
      "source": [
        "# AUC score\n",
        "metrics.auc(fpr_lr,tpr_lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gHHurD8Ii0Oq"
      },
      "source": [
        "# Part 4: Feature Importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dSx4TPO-i0Or"
      },
      "source": [
        "### Part 4.1:  Logistic Regression Model - Feature Selection Discussion "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BtLHUixoi0Ot"
      },
      "source": [
        "The corelated features that we are interested in"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbNTNeb7saCy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_with_corr = X.copy()\n",
        "X_with_corr['SalaryInRMB'] = X['EstimatedSalary'] * 6.91\n",
        "X_with_corr.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cQaXOIsUi0Ou",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# add L1 regularization to logistic regression\n",
        "# check the coef for feature selection\n",
        "scaler = StandardScaler()\n",
        "X_l1 = scaler.fit_transform(X_with_corr)\n",
        "LRmodel_l1 = LogisticRegression(penalty=\"l1\", C = 0.01, solver='liblinear')\n",
        "LRmodel_l1.fit(X_l1, y)\n",
        "\n",
        "indices = np.argsort(abs(LRmodel_l1.coef_[0]))[::-1]\n",
        "\n",
        "print (\"Logistic Regression (L1) Coefficients\")\n",
        "for ind in range(X_with_corr.shape[1]):\n",
        "  print (\"{0} : {1}\".format(X_with_corr.columns[indices[ind]],round(LRmodel_l1.coef_[0][indices[ind]], 4)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "majifZZqi0O9",
        "colab": {}
      },
      "source": [
        "# add L2 regularization to logistic regression\n",
        "# check the coef for feature selection\n",
        "np.random.seed()\n",
        "scaler = StandardScaler()\n",
        "X_l2 = scaler.fit_transform(X_with_corr)\n",
        "LRmodel_l2 = LogisticRegression(penalty=\"l2\", C = 0.1, solver='liblinear', random_state=42)\n",
        "LRmodel_l2.fit(X_l2, y)\n",
        "LRmodel_l2.coef_[0]\n",
        "\n",
        "indices = np.argsort(abs(LRmodel_l2.coef_[0]))[::-1]\n",
        "\n",
        "print (\"Logistic Regression (L2) Coefficients\")\n",
        "for ind in range(X_with_corr.shape[1]):\n",
        "  print (\"{0} : {1}\".format(X_with_corr.columns[indices[ind]],round(LRmodel_l2.coef_[0][indices[ind]], 4)))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uqs41ydLi0O_"
      },
      "source": [
        "### Part 4.2:  Random Forest Model - Feature Importance Discussion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MPxUM2lei0PA",
        "colab": {}
      },
      "source": [
        "# check feature importance of random forest for feature selection\n",
        "forest = RandomForestClassifier()\n",
        "forest.fit(X, y)\n",
        "\n",
        "importances = forest.feature_importances_\n",
        "\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "# Print the feature ranking\n",
        "print(\"Feature importance ranking by Random Forest Model:\")\n",
        "for ind in range(X.shape[1]):\n",
        "  print (\"{0} : {1}\".format(X.columns[indices[ind]],round(importances[indices[ind]], 4)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}